{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-28T16:21:42.000168Z","iopub.execute_input":"2023-09-28T16:21:42.000613Z","iopub.status.idle":"2023-09-28T16:21:42.009498Z","shell.execute_reply.started":"2023-09-28T16:21:42.000579Z","shell.execute_reply":"2023-09-28T16:21:42.008237Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"/kaggle/input/unit-3-nlp-txt-classification/sample_submission.csv\n/kaggle/input/unit-3-nlp-txt-classification/train.csv\n/kaggle/input/unit-3-nlp-txt-classification/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Общее описание решения и вывод:**\n\n1. При построении своей архитектуры использовал 2 вида RNN: Long Short-Term Memory (LSTM) и Gated Recurrent Units (GRU)\n\n2. Библиотека: PyTorch\n\n3. Рассматривал undirectional и bidirectional модели\n\n4. Был выполнен предварительный анализ текстов, на основании его результатов выбраны методы предобработки, связанные с очисткой текста от спецсимволов, знаков препинания, смайлов и др. Полное перечисление шагов предобработки содержится в описании ячейки.\n\n5. Анализ результатов влияния предобработки текста на результат показали, что лемматизация (независимо от количества лемматизируемых частей речи) ухудшает метрику, поэтому было принято решения отключить ее\n\n6. Первоначально характеристики модели подбирались на одном наборе датасетов train, valid, test в соотношении 0.7-0.2-0.1 с фиксированным RandomSeed. Это позволяло прогонять за раз несколько вариантов моделей с разными гиперпараметрами и сравнивать метрики на test между собой. При этом обучение одной модели на GPU T4 x2 от kaggle в таких условиях занимало 2-3мин на эпоху для LSTM и 2-3мин на эпоху для GRU, что позволяло реализовывать перебор выделенными kaggle ресурсами.\n\n7. При обучении моделей применялся lr_scheduler с линейным изменением шага, а также L2-регуляризация (через задание weight_decay > 0 в Adam-оптимизаторе. Согласно https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch реализация weight_decay в PyTorch фактически соответствует L2-регуляризации) и dropout. dropout_rate был подобран в ходе экспериментов с моделями. В ходе разработки были опробованы варинты с weight_decay > 0 и weight_decay = 0. Получено, что задание weight_decay = 1e-5 улучшает valid-метрику на 2-3%.\n\n8. Лучше всего на test датасете показали себя модели LSTM-bidirectional и GRU-undirectional.\n\n9. Для генерации submit применялся ансамбль из двух моделей, показавших лучший результат на test датасете. Обе модели перед предсказанием submit обучались на полном датасете, без разделения на train, valid, test. Полученные на submit-датасете предсказания осреднялись. Обучение каждой из моделей на полном датасете заняло 3-4мин на эпоху.\n\n10. Общие выводы: лучше всего показала себя LSTM-bidirectional модель. Лемматизация в случае RNN может ухудшать метрику, нельзя включать ее по умолчанию, без анализа влияния на результат. Наличие/отсутствие регуляризации может существенно повлиять на результат.\n\n11. Идея на будущее: в идеале можно было бы попробовать нарастить датасет с помощью аугментации, например через замену слов синонимами и перестановку слов, но сходу не получилось поднять библиотеки в kaggle.","metadata":{}},{"cell_type":"markdown","source":"## 1. Работа с данными","metadata":{}},{"cell_type":"code","source":"work_dir = '/kaggle/input/unit-3-nlp-txt-classification'","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.011685Z","iopub.execute_input":"2023-09-28T16:21:42.012939Z","iopub.status.idle":"2023-09-28T16:21:42.018803Z","shell.execute_reply.started":"2023-09-28T16:21:42.012898Z","shell.execute_reply":"2023-09-28T16:21:42.017548Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Загружаем train датасет, удаляя колонку с номером\n\ntrain_df = pd.read_csv(f'{work_dir}/train.csv')\ntrain_df = train_df.drop(['Unnamed: 0'], axis=1)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.020869Z","iopub.execute_input":"2023-09-28T16:21:42.021660Z","iopub.status.idle":"2023-09-28T16:21:42.178006Z","shell.execute_reply.started":"2023-09-28T16:21:42.021623Z","shell.execute_reply":"2023-09-28T16:21:42.176809Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"                                                    Text           Sentiment\n0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n1      advice Talk to your neighbours family to excha...            Positive\n2      Coronavirus Australia: Woolworths to give elde...            Positive\n3      My food stock is not the only one which is emp...            Positive\n4      Me, ready to go at supermarket during the #COV...  Extremely Negative\n...                                                  ...                 ...\n41154  Airline pilots offering to stock supermarket s...             Neutral\n41155  Response to complaint not provided citing COVI...  Extremely Negative\n41156  You know itÂs getting tough when @KameronWild...            Positive\n41157  Is it wrong that the smell of hand sanitizer i...             Neutral\n41158  @TartiiCat Well new/used Rift S are going for ...            Negative\n\n[41159 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>Airline pilots offering to stock supermarket s...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>Response to complaint not provided citing COVI...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>41156</th>\n      <td>You know itÂs getting tough when @KameronWild...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>41157</th>\n      <td>Is it wrong that the smell of hand sanitizer i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41158</th>\n      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>41159 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Проверяем на наличие nan\n\nnan_count = train_df.isna().sum().sum()\nnan_count","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.181390Z","iopub.execute_input":"2023-09-28T16:21:42.181842Z","iopub.status.idle":"2023-09-28T16:21:42.199118Z","shell.execute_reply.started":"2023-09-28T16:21:42.181807Z","shell.execute_reply":"2023-09-28T16:21:42.197833Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"# Удаляем записи с nan с снова проверяем на наличие. nan больше нет\n\nfixed_train_df = train_df.dropna()\nnan_count = fixed_train_df.isna().sum().sum()\nnan_count","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.201347Z","iopub.execute_input":"2023-09-28T16:21:42.202169Z","iopub.status.idle":"2023-09-28T16:21:42.233605Z","shell.execute_reply.started":"2023-09-28T16:21:42.202135Z","shell.execute_reply":"2023-09-28T16:21:42.232495Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Смотрим распределение по классам.\n# Видим, что тексты по классам распределены неравномерно, это нужно будет учесть при разбиении на train, val и test датасеты\n\nfixed_train_df['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.235269Z","iopub.execute_input":"2023-09-28T16:21:42.235756Z","iopub.status.idle":"2023-09-28T16:21:42.250755Z","shell.execute_reply.started":"2023-09-28T16:21:42.235719Z","shell.execute_reply":"2023-09-28T16:21:42.249690Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"Sentiment\nPositive              11422\nNegative               9917\nNeutral                7711\nExtremely Positive     6624\nExtremely Negative     5481\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Задаем словарь для нумерации классов и заменяем названия на номера\n\nclass_to_idx = {'Extremely Negative': 0,\n                 'Negative': 1,\n                 'Neutral': 2,\n                 'Positive': 3,\n                 'Extremely Positive': 4\n                }\n\ndef change_labels(input_label):\n    return class_to_idx[input_label]\n\nfixed_train_df['Sentiment'] = fixed_train_df['Sentiment'].map(change_labels)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.252545Z","iopub.execute_input":"2023-09-28T16:21:42.253279Z","iopub.status.idle":"2023-09-28T16:21:42.295609Z","shell.execute_reply.started":"2023-09-28T16:21:42.253239Z","shell.execute_reply":"2023-09-28T16:21:42.294565Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3703279817.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  fixed_train_df['Sentiment'] = fixed_train_df['Sentiment'].map(change_labels)\n","output_type":"stream"}]},{"cell_type":"code","source":"fixed_train_df['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.297266Z","iopub.execute_input":"2023-09-28T16:21:42.297616Z","iopub.status.idle":"2023-09-28T16:21:42.309122Z","shell.execute_reply.started":"2023-09-28T16:21:42.297582Z","shell.execute_reply":"2023-09-28T16:21:42.307864Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"Sentiment\n3    11422\n1     9917\n2     7711\n4     6624\n0     5481\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Проверяем тексты на наличие emojis\n\nimport emoji\n\ndef extract_emojis(input_text):    \n    return [match[\"emoji\"] for word in input_text for match in emoji.emoji_list(word)]","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.310973Z","iopub.execute_input":"2023-09-28T16:21:42.311986Z","iopub.status.idle":"2023-09-28T16:21:42.317556Z","shell.execute_reply.started":"2023-09-28T16:21:42.311955Z","shell.execute_reply":"2023-09-28T16:21:42.316499Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Видим, что emojis, которые могли бы повлиять на смысловую окраску в текстах нет. Найденные эмоджи можно удалить\n\ntext_emojis = fixed_train_df['Text'].map(extract_emojis)\ntext_emojis.explode().value_counts().nlargest(25)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:21:42.324441Z","iopub.execute_input":"2023-09-28T16:21:42.324692Z","iopub.status.idle":"2023-09-28T16:22:12.417625Z","shell.execute_reply.started":"2023-09-28T16:21:42.324668Z","shell.execute_reply":"2023-09-28T16:22:12.416407Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"Text\n©    65\n®     5\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Словарь для преобразования emoticons из https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n\nEMOTICONS_EMO = {\n    u\":‑)\":\"Happy face or smiley\",\n    u\":-))\":\"Very Happy face or smiley\",\n    u\":-)))\":\"Very very Happy face or smiley\",\n    u\":)\":\"Happy face or smiley\",\n    u\":))\":\"Very Happy face or smiley\",\n    u\":)))\":\"Very very Happy face or smiley\",\n    u\":-]\":\"Happy face or smiley\",\n    u\":]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-)\":\"Happy face smiley\",\n    u\":o)\":\"Happy face smiley\",\n    u\":-}\":\"Happy face smiley\",\n    u\":}\":\"Happy face smiley\",\n    u\":-)\":\"Happy face smiley\",\n    u\":c)\":\"Happy face smiley\",\n    u\":^)\":\"Happy face smiley\",\n    u\"=]\":\"Happy face smiley\",\n    u\"=)\":\"Happy face smiley\",\n    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-))\":\"Very happy\",\n    u\":-(\":\"Frown, sad, angry or pouting\",\n    u\":‑(\":\"Frown, sad, angry or pouting\",\n    u\":(\":\"Frown, sad, angry or pouting\",\n    u\":‑c\":\"Frown, sad, angry or pouting\",\n    u\":c\":\"Frown, sad, angry or pouting\",\n    u\":‑<\":\"Frown, sad, angry or pouting\",\n    u\":<\":\"Frown, sad, angry or pouting\",\n    u\":‑[\":\"Frown, sad, angry or pouting\",\n    u\":[\":\"Frown, sad, angry or pouting\",\n    u\":-||\":\"Frown, sad, angry or pouting\",\n    u\">:[\":\"Frown, sad, angry or pouting\",\n    u\":{\":\"Frown, sad, angry or pouting\",\n    u\":@\":\"Frown, sad, angry or pouting\",\n    u\">:(\":\"Frown, sad, angry or pouting\",\n    u\":'‑(\":\"Crying\",\n    u\":'(\":\"Crying\",\n    u\":'‑)\":\"Tears of happiness\",\n    u\":')\":\"Tears of happiness\",\n    u\"D‑':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":‑O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":‑o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8‑0\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-*\":\"Kiss\",\n    u\":*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";‑)\":\"Wink or smirk\",\n    u\";)\":\"Wink or smirk\",\n    u\"*-)\":\"Wink or smirk\",\n    u\"*)\":\"Wink or smirk\",\n    u\";‑]\":\"Wink or smirk\",\n    u\";]\":\"Wink or smirk\",\n    u\";^)\":\"Wink or smirk\",\n    u\":‑,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‑|\":\"Straight face\",\n    u\":|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‑)\":\"Angel, saint or innocent\",\n    u\"O:)\":\"Angel, saint or innocent\",\n    u\"0:‑3\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:‑)\":\"Angel, saint or innocent\",\n    u\"0:)\":\"Angel, saint or innocent\",\n    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;^)\":\"Angel, saint or innocent\",\n    u\">:‑)\":\"Evil or devilish\",\n    u\">:)\":\"Evil or devilish\",\n    u\"}:‑)\":\"Evil or devilish\",\n    u\"}:)\":\"Evil or devilish\",\n    u\"3:‑)\":\"Evil or devilish\",\n    u\"3:)\":\"Evil or devilish\",\n    u\">;)\":\"Evil or devilish\",\n    u\"|;‑)\":\"Cool\",\n    u\"|‑O\":\"Bored\",\n    u\":‑J\":\"Tongue-in-cheek\",\n    u\"#‑)\":\"Party all night\",\n    u\"%‑)\":\"Drunk or confused\",\n    u\"%)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:‑|\":\"Dump\",\n    u\"(>_<)\":\"Troubled\",\n    u\"(>_<)>\":\"Troubled\",\n    u\"(';')\":\"Baby\",\n    u\"(^^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"(^_^;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"(-_-;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"(~_~;) (・.・;)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"(-_-)zzz\":\"Sleeping\",\n    u\"(^_-)\":\"Wink\",\n    u\"((+_+))\":\"Confused\",\n    u\"(+o+)\":\"Confused\",\n    u\"(o|o)\":\"Ultraman\",\n    u\"^_^\":\"Joyful\",\n    u\"(^_^)/\":\"Joyful\",\n    u\"(^O^)／\":\"Joyful\",\n    u\"(^o^)／\":\"Joyful\",\n    u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"('_')\":\"Sad or Crying\",\n    u\"(/_;)\":\"Sad or Crying\",\n    u\"(T_T) (;_;)\":\"Sad or Crying\",\n    u\"(;_;\":\"Sad of Crying\",\n    u\"(;_:)\":\"Sad or Crying\",\n    u\"(;O;)\":\"Sad or Crying\",\n    u\"(:_;)\":\"Sad or Crying\",\n    u\"(ToT)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q.Q\":\"Sad or Crying\",\n    u\"T.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"(-.-)\":\"Shame\",\n    u\"(-_-)\":\"Shame\",\n    u\"(一一)\":\"Shame\",\n    u\"(；一_一)\":\"Shame\",\n    u\"(=_=)\":\"Tired\",\n    u\"(=^·^=)\":\"cat\",\n    u\"(=^··^=)\":\"cat\",\n    u\"=_^= \":\"cat\",\n    u\"(..)\":\"Looking down\",\n    u\"(._.)\":\"Looking down\",\n    u\"^m^\":\"Giggling with hand covering mouth\",\n    u\"(・・?\":\"Confusion\",\n    u\"(?_?)\":\"Confusion\",\n    u\">^_^<\":\"Normal Laugh\",\n    u\"<^!^>\":\"Normal Laugh\",\n    u\"^/^\":\"Normal Laugh\",\n    u\"（*^_^*）\" :\"Normal Laugh\",\n    u\"(^<^) (^.^)\":\"Normal Laugh\",\n    u\"(^^)\":\"Normal Laugh\",\n    u\"(^.^)\":\"Normal Laugh\",\n    u\"(^_^.)\":\"Normal Laugh\",\n    u\"(^_^)\":\"Normal Laugh\",\n    u\"(^^)\":\"Normal Laugh\",\n    u\"(^J^)\":\"Normal Laugh\",\n    u\"(*^.^*)\":\"Normal Laugh\",\n    u\"(^—^）\":\"Normal Laugh\",\n    u\"(#^.^#)\":\"Normal Laugh\",\n    u\"（^—^）\":\"Waving\",\n    u\"(;_;)/~~~\":\"Waving\",\n    u\"(^.^)/~~~\":\"Waving\",\n    u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n    u\"(T_T)/~~~\":\"Waving\",\n    u\"(ToT)/~~~\":\"Waving\",\n    u\"(*^0^*)\":\"Excited\",\n    u\"(*_*)\":\"Amazed\",\n    u\"(*_*;\":\"Amazed\",\n    u\"(+_+) (@_@)\":\"Amazed\",\n    u\"(*^^)v\":\"Laughing,Cheerful\",\n    u\"(^_^)v\":\"Laughing,Cheerful\",\n    u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n    u'(-\"-)':\"Worried\",\n    u\"(ーー;)\":\"Worried\",\n    u\"(^0_0^)\":\"Eyeglasses\",\n    u\"(＾ｖ＾)\":\"Happy\",\n    u\"(＾ｕ＾)\":\"Happy\",\n    u\"(^)o(^)\":\"Happy\",\n    u\"(^O^)\":\"Happy\",\n    u\"(^o^)\":\"Happy\",\n    u\")^o^(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o.O\":\"Surpised\",\n    u\"(o.o)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"(*￣m￣)\":\"Dissatisfied\",\n    u\"(‘A`)\":\"Snubbed or Deflated\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:12.419471Z","iopub.execute_input":"2023-09-28T16:22:12.419989Z","iopub.status.idle":"2023-09-28T16:22:12.450893Z","shell.execute_reply.started":"2023-09-28T16:22:12.419940Z","shell.execute_reply":"2023-09-28T16:22:12.449531Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Проверяем текст на наличие emoticons\n\nimport re\n\ndef extract_emoticons(input_text):\n    emoticon_pattern = re.compile(re.escape(u'(' + u'|'.join(k for k in EMOTICONS_EMO) + u')'))\n    return re.findall(emoticon_pattern, input_text)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:12.452303Z","iopub.execute_input":"2023-09-28T16:22:12.452643Z","iopub.status.idle":"2023-09-28T16:22:12.460616Z","shell.execute_reply.started":"2023-09-28T16:22:12.452603Z","shell.execute_reply":"2023-09-28T16:22:12.459382Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Видим, что emoticons, которые представлены в словаре, в тексте нет\n\ntext_emoticons = fixed_train_df['Text'].map(extract_emoticons)\ntext_emoticons.explode().value_counts().nlargest(25)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:12.462656Z","iopub.execute_input":"2023-09-28T16:22:12.463466Z","iopub.status.idle":"2023-09-28T16:22:18.482893Z","shell.execute_reply.started":"2023-09-28T16:22:12.463430Z","shell.execute_reply":"2023-09-28T16:22:18.481913Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"Series([], Name: count, dtype: int64)"},"metadata":{}}]},{"cell_type":"code","source":"# Импортируем словарь для преобразования слэнговых сокращений\n\nimport requests\n\nchat_words_map_dict = {}\nchat_words_list = []\n\nresponse = requests.get(\"https://raw.githubusercontent.com/rishabhverma17/sms_slang_translator/master/slang.txt\")\n\nfor line in response.text.split(\"\\n\"):\n    if line != \"\" and \"=\" in line:\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:18.484534Z","iopub.execute_input":"2023-09-28T16:22:18.484929Z","iopub.status.idle":"2023-09-28T16:22:18.657199Z","shell.execute_reply.started":"2023-09-28T16:22:18.484892Z","shell.execute_reply":"2023-09-28T16:22:18.656204Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Загружаем библиотеки для предобработки текста\n\nimport nltk\nimport subprocess\n\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:18.659109Z","iopub.execute_input":"2023-09-28T16:22:18.660127Z","iopub.status.idle":"2023-09-28T16:22:18.722266Z","shell.execute_reply.started":"2023-09-28T16:22:18.660088Z","shell.execute_reply":"2023-09-28T16:22:18.721286Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n","output_type":"stream"},{"name":"stderr","text":"replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Пишем функции для предобработки текста.\n# За основу брал функции из статьи https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n# Как было показано выше, смысловых emojis в тексте нет, поэтому удаляем их\n# Поскольку emoticon вносят вклад в эмоциональную окраску текста, а наша цель - классифицировать тексты\n#    именно по эмоциональной окраске, я не стал их удалять, а заменил на слова\n#    сделать корректировку правописания не получилось т.к. spellchecker не поднялся нормально на kaggle, постоянно выдает ошибки при импорте\n\n# Итого препроцессинг состоит из следующих последовательных этапов\n#    1. Переводим слова с нижний регистр  \n#    2. Удаляем знаки препинания\n#    3. Удаляем стоп-слова\n#    4. Лемматизация текста. При \n#    5. Удаляем emojis\n#    6. Удаляем куски url\n#    7. Удаляем html тэги\n#    8. Преобразовываем слэнговые сокращения\n\nfrom tqdm import tqdm\n\nfrom bs4 import BeautifulSoup\n\nimport string\n\nimport re\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\n# Попытки подобрать параметры лемматизации, улучшающие результат\n# Включал и выключал отдельные части речи, однако лучший результат все равно получен без лемматизации\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# wordnet_map = {\"N\":wordnet.NOUN}\n\ndef remove_punctuation(input_text):\n    return \" \".join([word for word in str(input_text).split() if word not in set(string.punctuation)])\n\ndef remove_stopwords(input_text):\n    return \" \".join([word for word in str(input_text).split() if word not in stopwords])\n\ndef lemmatize_words(input_text):\n    pos_tagged_text = nltk.pos_tag(input_text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndef remove_emojis(input_text):\n    return emoji.replace_emoji(input_text, replace='')\n\ndef remove_urls(input_text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', input_text)\n\ndef remove_html(input_text):\n    return BeautifulSoup(input_text, \"lxml\").text\n\ndef chat_words_conversion(input_text):\n    new_text = []\n    for w in input_text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\ndef process_text(input_text):\n    processed_text = input_text.lower() # Переводим слова с нижний регистр  \n    processed_text = remove_punctuation(processed_text) # Удаляем знаки препинания\n    processed_text = remove_stopwords(processed_text) # Удаляем \n    # processed_text = lemmatize_words(processed_text) # Лемматизация текста. Отключена т.к. ухудшает результат\n    processed_text = remove_emojis(processed_text) # Удаляем emojis\n    processed_text = remove_urls(processed_text) # Удаляем куски url\n    processed_text = remove_html(processed_text) # Удаляем html тэги\n    processed_text = chat_words_conversion(processed_text) # Преобразовываем слэнговые сокращения\n    return processed_text\n\ntqdm.pandas()\n\nfixed_train_df['Processed_text'] = fixed_train_df['Text'].progress_map(process_text)\nfixed_train_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:18.724000Z","iopub.execute_input":"2023-09-28T16:22:18.724704Z","iopub.status.idle":"2023-09-28T16:22:48.775122Z","shell.execute_reply.started":"2023-09-28T16:22:18.724668Z","shell.execute_reply":"2023-09-28T16:22:48.773810Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"  0%|          | 0/41155 [00:00<?, ?it/s]/tmp/ipykernel_28/3451095874.py:53: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  return BeautifulSoup(input_text, \"lxml\").text\n100%|██████████| 41155/41155 [00:30<00:00, 1371.10it/s]\n/tmp/ipykernel_28/3451095874.py:77: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  fixed_train_df['Processed_text'] = fixed_train_df['Text'].progress_map(process_text)\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                                                    Text  Sentiment  \\\n0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...          2   \n1      advice Talk to your neighbours family to excha...          3   \n2      Coronavirus Australia: Woolworths to give elde...          3   \n3      My food stock is not the only one which is emp...          3   \n4      Me, ready to go at supermarket during the #COV...          0   \n...                                                  ...        ...   \n41154  Airline pilots offering to stock supermarket s...          2   \n41155  Response to complaint not provided citing COVI...          0   \n41156  You know itÂs getting tough when @KameronWild...          3   \n41157  Is it wrong that the smell of hand sanitizer i...          2   \n41158  @TartiiCat Well new/used Rift S are going for ...          1   \n\n                                          Processed_text  \n0                        @menyrbie @phil_gahan @chrisitv  \n1      advice talk neighbours family exchange phone n...  \n2      coronavirus australia: woolworths give elderly...  \n3      food stock one empty... please, panic, enough ...  \n4      me, ready go supermarket #covid19 outbreak. i'...  \n...                                                  ...  \n41154  airline pilots offering stock supermarket shel...  \n41155  response complaint provided citing covid-19 re...  \n41156  know itâs getting tough @kameronwilds rationi...  \n41157  wrong smell hand sanitizer starting turn on? #...  \n41158  @tartiicat well new/used rift going $700.00 am...  \n\n[41155 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Sentiment</th>\n      <th>Processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>2</td>\n      <td>@menyrbie @phil_gahan @chrisitv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>3</td>\n      <td>advice talk neighbours family exchange phone n...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>3</td>\n      <td>coronavirus australia: woolworths give elderly...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>3</td>\n      <td>food stock one empty... please, panic, enough ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>0</td>\n      <td>me, ready go supermarket #covid19 outbreak. i'...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>Airline pilots offering to stock supermarket s...</td>\n      <td>2</td>\n      <td>airline pilots offering stock supermarket shel...</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>Response to complaint not provided citing COVI...</td>\n      <td>0</td>\n      <td>response complaint provided citing covid-19 re...</td>\n    </tr>\n    <tr>\n      <th>41156</th>\n      <td>You know itÂs getting tough when @KameronWild...</td>\n      <td>3</td>\n      <td>know itâs getting tough @kameronwilds rationi...</td>\n    </tr>\n    <tr>\n      <th>41157</th>\n      <td>Is it wrong that the smell of hand sanitizer i...</td>\n      <td>2</td>\n      <td>wrong smell hand sanitizer starting turn on? #...</td>\n    </tr>\n    <tr>\n      <th>41158</th>\n      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n      <td>1</td>\n      <td>@tartiicat well new/used rift going $700.00 am...</td>\n    </tr>\n  </tbody>\n</table>\n<p>41155 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Разделяем на train, valid и test выборки\n# train используем для обучения модели, valid для валидации\n# test модель не видит в процессе обучения, ее используем для сравнения моделей\n# преобразуем в список полный датасет чтобы обучить на нем наилучшую модель для получения submit\n\nfrom sklearn.model_selection import train_test_split\n\nRANDOM_SEED = 42\n\nTEST_SPLIT_SIZE = 0.1\nVALID_SPLIT_SIZE = 0.2\n\nX_rem, X_test, y_rem, y_test = train_test_split(fixed_train_df['Processed_text'].tolist(),\n                                                fixed_train_df['Sentiment'].tolist(),\n                                                test_size=TEST_SPLIT_SIZE,\n                                                shuffle= True,\n                                                stratify=fixed_train_df['Sentiment'].tolist(),\n                                                random_state=RANDOM_SEED)\nX_train, X_val, y_train, y_val = train_test_split(X_rem,\n                                                  y_rem,\n                                                  test_size=VALID_SPLIT_SIZE/(1.0-TEST_SPLIT_SIZE),\n                                                  shuffle= True,\n                                                  stratify=y_rem,\n                                                  random_state=RANDOM_SEED)\n\ntrain_data = list(zip(X_train, y_train))\ntest_data = list(zip(X_test, y_test))\nvalid_data = list(zip(X_val, y_val))\n\ncomplete_data = list(zip(fixed_train_df['Processed_text'].tolist(), fixed_train_df['Sentiment'].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:48.777066Z","iopub.execute_input":"2023-09-28T16:22:48.777512Z","iopub.status.idle":"2023-09-28T16:22:48.895552Z","shell.execute_reply.started":"2023-09-28T16:22:48.777474Z","shell.execute_reply":"2023-09-28T16:22:48.894504Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Проверяем, что разделение верное\n\nprint(f'fixed_train_df.shape: {fixed_train_df.shape[0]}')\nprint(f'train_data size: {len(train_data)}')\nprint(f'test_data size: {len(test_data)}')\nprint(f'valid_data size: {len(valid_data)}')\nprint(f'summ: {len(train_data)+len(test_data)+len(valid_data)}')\nprint(f'complete_data size: {len(complete_data)}')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:48.896997Z","iopub.execute_input":"2023-09-28T16:22:48.897380Z","iopub.status.idle":"2023-09-28T16:22:48.904271Z","shell.execute_reply.started":"2023-09-28T16:22:48.897344Z","shell.execute_reply":"2023-09-28T16:22:48.903282Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"fixed_train_df.shape: 41155\ntrain_data size: 28808\ntest_data size: 4116\nvalid_data size: 8231\nsumm: 41155\ncomplete_data size: 41155\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Векторизация текстов","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchtext.data.utils import get_tokenizer\n\nfrom torchtext.vocab import build_vocab_from_iterator\n\nimport spacy\n\ntorch.manual_seed(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntokenizer = get_tokenizer(\"basic_english\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:48.905611Z","iopub.execute_input":"2023-09-28T16:22:48.906704Z","iopub.status.idle":"2023-09-28T16:22:48.916919Z","shell.execute_reply.started":"2023-09-28T16:22:48.906665Z","shell.execute_reply":"2023-09-28T16:22:48.916270Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Пишем функцию для токенизации текста и составляем словарь\n\ndef get_tokens(datasets):\n    for dataset in datasets:\n        for text, sentiment in dataset:\n            yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(\n    get_tokens([train_data, test_data, valid_data]),\n    min_freq=1,\n    specials=['<unk>'],\n    special_first=True\n)\n\nvocab.set_default_index(vocab['<unk>'])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:48.918252Z","iopub.execute_input":"2023-09-28T16:22:48.919431Z","iopub.status.idle":"2023-09-28T16:22:50.388150Z","shell.execute_reply.started":"2023-09-28T16:22:48.919373Z","shell.execute_reply":"2023-09-28T16:22:50.387065Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"text_pipeline = lambda x: vocab(tokenizer(x))\nlabel_pipeline = lambda x: int(x)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.389540Z","iopub.execute_input":"2023-09-28T16:22:50.389904Z","iopub.status.idle":"2023-09-28T16:22:50.398038Z","shell.execute_reply.started":"2023-09-28T16:22:50.389871Z","shell.execute_reply":"2023-09-28T16:22:50.397189Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Пишем функцию для векторизации батча.\n# Векторы текстов приводим к одной длине, заполняя недостающие слова как '<unk>' с помощью pad_sequence\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    for _text, _label in batch:\n        label_list.append(label_pipeline(_label))\n        processed_text = text_pipeline(_text)\n        text_list.append(processed_text)\n    text_list = pad_sequence([torch.tensor(p) for p in text_list], batch_first=True)\n    label_list = torch.tensor(label_list, dtype=torch.int32)\n    return text_list.to(device), label_list.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.400942Z","iopub.execute_input":"2023-09-28T16:22:50.402060Z","iopub.status.idle":"2023-09-28T16:22:50.409606Z","shell.execute_reply.started":"2023-09-28T16:22:50.402025Z","shell.execute_reply":"2023-09-28T16:22:50.408454Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Создаем итераторы-загрузочники для каждого из датасетов\n\nBATCH_SIZE = 64\n\ntrain_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=True)\nvalid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=False)\ntest_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.411259Z","iopub.execute_input":"2023-09-28T16:22:50.412021Z","iopub.status.idle":"2023-09-28T16:22:50.422772Z","shell.execute_reply.started":"2023-09-28T16:22:50.411985Z","shell.execute_reply":"2023-09-28T16:22:50.421964Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Проверяем работу итераторов\n\nprint('Train')\nfor batch in train_dataloader:\n    print(f'Text matrix size: {batch[0].size()}')\n    print(f'Target vector size: {batch[1].size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_dataloader:\n    print(f'Text matrix size: {batch[0].size()}')\n    print(f'Target vector size: {batch[1].size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_dataloader:\n    print(f'Text matrix size: {batch[0].size()}')\n    print(f'Target vector size: {batch[1].size()}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.424508Z","iopub.execute_input":"2023-09-28T16:22:50.425345Z","iopub.status.idle":"2023-09-28T16:22:50.451718Z","shell.execute_reply.started":"2023-09-28T16:22:50.425301Z","shell.execute_reply":"2023-09-28T16:22:50.450828Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Train\nText matrix size: torch.Size([64, 47])\nTarget vector size: torch.Size([64])\n\nValid:\nText matrix size: torch.Size([64, 45])\nTarget vector size: torch.Size([64])\n\nTest:\nText matrix size: torch.Size([64, 45])\nTarget vector size: torch.Size([64])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Разработка и анализ моделей","metadata":{}},{"cell_type":"code","source":"# Задаем архитектуру LSTM модели\n\nimport torch.nn as nn\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 embedding_dim,\n                 hidden_dim,\n                 output_dim,\n                 n_layers,\n                 bidirectional,\n                 dropout_rate):\n        \n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size,\n                                      embedding_dim)\n        \n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_dim,\n                            n_layers,\n                            bidirectional=bidirectional,\n                            dropout=dropout_rate,\n                            batch_first=True)\n        \n        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, X_batch, length):\n        embeddings = self.dropout(self.embedding(X_batch))\n        \n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeddings,\n                                                            length.to('cpu'),\n                                                            batch_first=True, \n                                                            enforce_sorted=False)\n        \n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n       \n        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n        \n        if self.lstm.bidirectional:\n            hidden = self.dropout(torch.cat([hidden[-1,:,:], hidden[-2,:,:]], dim=-1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n            \n        prediction = self.linear(hidden)\n        \n        return prediction","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.453239Z","iopub.execute_input":"2023-09-28T16:22:50.453791Z","iopub.status.idle":"2023-09-28T16:22:50.465272Z","shell.execute_reply.started":"2023-09-28T16:22:50.453757Z","shell.execute_reply":"2023-09-28T16:22:50.464045Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Задаем гиперпараметры LSTM модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = len(class_to_idx)\nN_LAYERS = 2\nBIDIRECTIONAL = False\nDROPOUT = 0.4\n\nlstm_model = LSTMClassifier(INPUT_DIM,\n                            EMBEDDING_DIM,\n                            HIDDEN_DIM,\n                            OUTPUT_DIM,\n                            N_LAYERS,\n                            BIDIRECTIONAL,\n                            DROPOUT)\nlstm_model = lstm_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.466851Z","iopub.execute_input":"2023-09-28T16:22:50.467777Z","iopub.status.idle":"2023-09-28T16:22:50.564707Z","shell.execute_reply.started":"2023-09-28T16:22:50.467740Z","shell.execute_reply":"2023-09-28T16:22:50.563614Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# Задаем гиперпараметры LSTM-Bidirectional модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = len(class_to_idx)\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.4\n\nlstm_bi_model = LSTMClassifier(INPUT_DIM,\n                               EMBEDDING_DIM,\n                               HIDDEN_DIM,\n                               OUTPUT_DIM,\n                               N_LAYERS,\n                               BIDIRECTIONAL,\n                               DROPOUT)\nlstm_bi_model = lstm_bi_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.566288Z","iopub.execute_input":"2023-09-28T16:22:50.566686Z","iopub.status.idle":"2023-09-28T16:22:50.663481Z","shell.execute_reply.started":"2023-09-28T16:22:50.566651Z","shell.execute_reply":"2023-09-28T16:22:50.662366Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Задаем архитектуру GRU модели\n\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass GRUClassifier(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 embedding_dim,\n                 hidden_dim,\n                 output_dim,\n                 n_layers,\n                 bidirectional,\n                 dropout_rate):\n        super(GRUClassifier, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n        self.gru = nn.GRU(input_size=embedding_dim,\n                          hidden_size=hidden_dim,\n                          num_layers=n_layers,\n                          bidirectional=bidirectional,\n                          dropout=dropout_rate,\n                          batch_first=True)\n        self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        # self.linear = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n\n    def forward(self, X_batch, length):\n        embeddings = self.dropout(self.embedding(X_batch))\n        \n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeddings,\n                                                            length.to('cpu'),\n                                                            batch_first=True, \n                                                            enforce_sorted=False)\n        \n        packed_output, hidden = self.gru(packed_embedded)\n       \n        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n        \n        #if self.gru.bidirectional:\n         #   last_tensor = self.dropout(torch.cat([output[-1,:,:], output[-2,:,:]], dim=-1))\n        #else:\n        last_tensor = self.dropout(output[-1,:,:])\n            \n        prediction = self.linear(last_tensor)\n        \n        return prediction","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.671407Z","iopub.execute_input":"2023-09-28T16:22:50.671768Z","iopub.status.idle":"2023-09-28T16:22:50.683606Z","shell.execute_reply.started":"2023-09-28T16:22:50.671737Z","shell.execute_reply":"2023-09-28T16:22:50.682377Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Задаем гиперпараметры GRU модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nN_LAYERS = 2\nBIDIRECTIONAL = False\nDROPOUT = 0.4\n\ngru_model = GRUClassifier(INPUT_DIM,\n                          EMBEDDING_DIM,\n                          HIDDEN_DIM,\n                          OUTPUT_DIM,\n                          N_LAYERS,\n                          BIDIRECTIONAL,\n                          DROPOUT)\ngru_model = gru_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.685544Z","iopub.execute_input":"2023-09-28T16:22:50.686277Z","iopub.status.idle":"2023-09-28T16:22:50.766253Z","shell.execute_reply.started":"2023-09-28T16:22:50.686041Z","shell.execute_reply":"2023-09-28T16:22:50.765075Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Задаем гиперпараметры GRU-Bidirectional модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.4\n\ngru_bi_model = GRUClassifier(INPUT_DIM,\n                             EMBEDDING_DIM,\n                             HIDDEN_DIM,\n                             OUTPUT_DIM,\n                             N_LAYERS,\n                             BIDIRECTIONAL,\n                             DROPOUT)\ngru_bi_model = gru_bi_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.767905Z","iopub.execute_input":"2023-09-28T16:22:50.768318Z","iopub.status.idle":"2023-09-28T16:22:50.854523Z","shell.execute_reply.started":"2023-09-28T16:22:50.768278Z","shell.execute_reply":"2023-09-28T16:22:50.853419Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# Пишем функцию для обучения модели\n\ndef train(model, iterator, optimizer, scheduler, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        labels = batch[1].type(torch.LongTensor).to(device)\n        \n        text, text_lengths = batch[0].to(device), torch.tensor([len(batch[0][0])] * len(batch[0])).to(device)\n        \n        predictions = model(text, text_lengths).squeeze(1)\n        \n        loss = criterion(predictions, labels)      \n        \n        optimizer.zero_grad()\n               \n        loss.backward()\n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n        y_pred_class = torch.argmax(torch.softmax(predictions, dim=1), dim=1)\n\n        epoch_loss += loss.item()\n        epoch_acc += (y_pred_class==labels).sum().item() / len(predictions)\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.856185Z","iopub.execute_input":"2023-09-28T16:22:50.856709Z","iopub.status.idle":"2023-09-28T16:22:50.868846Z","shell.execute_reply.started":"2023-09-28T16:22:50.856667Z","shell.execute_reply":"2023-09-28T16:22:50.867649Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Пишем функцию для валидации модели\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            labels = batch[1].type(torch.LongTensor).to(device)\n\n            text, text_lengths = batch[0].to(device), torch.tensor([len(batch[0][0])] * len(batch[0])).to(device)\n            \n            predictions = model(text, text_lengths).squeeze(1)\n            \n            loss = criterion(predictions, labels)\n            \n            y_pred_class = torch.argmax(torch.softmax(predictions, dim=1), dim=1)\n\n            epoch_loss += loss.item()\n            epoch_acc += (y_pred_class==labels).sum().item() / len(predictions)\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.870646Z","iopub.execute_input":"2023-09-28T16:22:50.875669Z","iopub.status.idle":"2023-09-28T16:22:50.887901Z","shell.execute_reply.started":"2023-09-28T16:22:50.875523Z","shell.execute_reply":"2023-09-28T16:22:50.886676Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\ndef train_model(train_dataloader,\n                model,\n                model_name,\n                optimizer,\n                scheduler,\n                criterion,\n                num_epochs\n               ):\n    best_valid_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler, criterion)\n        valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n        \n        end_time = time.time()\n        \n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), f\"best-model_{model_name}.pt\")\n            \n        print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n        print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.889694Z","iopub.execute_input":"2023-09-28T16:22:50.890305Z","iopub.status.idle":"2023-09-28T16:22:50.901712Z","shell.execute_reply.started":"2023-09-28T16:22:50.890266Z","shell.execute_reply":"2023-09-28T16:22:50.900285Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 20","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.904408Z","iopub.execute_input":"2023-09-28T16:22:50.905452Z","iopub.status.idle":"2023-09-28T16:22:50.911506Z","shell.execute_reply.started":"2023-09-28T16:22:50.905236Z","shell.execute_reply":"2023-09-28T16:22:50.910683Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# Задаем оптимизатор, шедулер, устанавливаем weight_decay > 0 для L2-регуляризации в Adam\n\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\nlearning_rate = 1e-3\nlstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate, weight_decay=1e-5)\nlstm_scheduler = lr_scheduler.LinearLR(lstm_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.913252Z","iopub.execute_input":"2023-09-28T16:22:50.913922Z","iopub.status.idle":"2023-09-28T16:22:50.921590Z","shell.execute_reply.started":"2023-09-28T16:22:50.913884Z","shell.execute_reply":"2023-09-28T16:22:50.920694Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Задаем функцию потерь для LSTM модели, переносим на device, обучаем модель\n\nlstm_criterion = nn.CrossEntropyLoss()\nlstm_criterion = lstm_criterion.to(device)\n\ntrain_model(train_dataloader,\n            lstm_model,\n            'lstm',\n            lstm_optimizer,\n            lstm_scheduler,\n            lstm_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:22:50.923320Z","iopub.execute_input":"2023-09-28T16:22:50.924069Z","iopub.status.idle":"2023-09-28T16:25:34.466053Z","shell.execute_reply.started":"2023-09-28T16:22:50.924033Z","shell.execute_reply":"2023-09-28T16:25:34.464097Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 8s\n\tTrain Loss: 1.565 | Train Acc: 27.85%\n\t Val. Loss: 1.531 |  Val. Acc: 31.01%\nEpoch: 02 | Epoch Time: 0m 8s\n\tTrain Loss: 1.514 | Train Acc: 31.16%\n\t Val. Loss: 1.504 |  Val. Acc: 32.29%\nEpoch: 03 | Epoch Time: 0m 8s\n\tTrain Loss: 1.483 | Train Acc: 32.54%\n\t Val. Loss: 1.454 |  Val. Acc: 33.87%\nEpoch: 04 | Epoch Time: 0m 8s\n\tTrain Loss: 1.435 | Train Acc: 35.30%\n\t Val. Loss: 1.410 |  Val. Acc: 37.16%\nEpoch: 05 | Epoch Time: 0m 8s\n\tTrain Loss: 1.389 | Train Acc: 38.26%\n\t Val. Loss: 1.354 |  Val. Acc: 40.49%\nEpoch: 06 | Epoch Time: 0m 8s\n\tTrain Loss: 1.293 | Train Acc: 43.80%\n\t Val. Loss: 1.222 |  Val. Acc: 48.80%\nEpoch: 07 | Epoch Time: 0m 8s\n\tTrain Loss: 1.189 | Train Acc: 49.87%\n\t Val. Loss: 1.122 |  Val. Acc: 54.06%\nEpoch: 08 | Epoch Time: 0m 8s\n\tTrain Loss: 1.103 | Train Acc: 54.27%\n\t Val. Loss: 1.061 |  Val. Acc: 56.97%\nEpoch: 09 | Epoch Time: 0m 8s\n\tTrain Loss: 1.028 | Train Acc: 57.91%\n\t Val. Loss: 0.994 |  Val. Acc: 61.08%\nEpoch: 10 | Epoch Time: 0m 7s\n\tTrain Loss: 0.952 | Train Acc: 62.03%\n\t Val. Loss: 0.928 |  Val. Acc: 64.43%\nEpoch: 11 | Epoch Time: 0m 8s\n\tTrain Loss: 0.879 | Train Acc: 65.33%\n\t Val. Loss: 0.928 |  Val. Acc: 65.71%\nEpoch: 12 | Epoch Time: 0m 8s\n\tTrain Loss: 0.812 | Train Acc: 68.85%\n\t Val. Loss: 0.845 |  Val. Acc: 68.14%\nEpoch: 13 | Epoch Time: 0m 8s\n\tTrain Loss: 0.749 | Train Acc: 71.48%\n\t Val. Loss: 0.824 |  Val. Acc: 70.27%\nEpoch: 14 | Epoch Time: 0m 8s\n\tTrain Loss: 0.702 | Train Acc: 73.40%\n\t Val. Loss: 0.810 |  Val. Acc: 71.40%\nEpoch: 15 | Epoch Time: 0m 8s\n\tTrain Loss: 0.640 | Train Acc: 76.21%\n\t Val. Loss: 0.790 |  Val. Acc: 72.15%\nEpoch: 16 | Epoch Time: 0m 8s\n\tTrain Loss: 0.596 | Train Acc: 77.75%\n\t Val. Loss: 0.821 |  Val. Acc: 71.64%\nEpoch: 17 | Epoch Time: 0m 8s\n\tTrain Loss: 0.556 | Train Acc: 79.88%\n\t Val. Loss: 0.787 |  Val. Acc: 73.34%\nEpoch: 18 | Epoch Time: 0m 8s\n\tTrain Loss: 0.512 | Train Acc: 81.55%\n\t Val. Loss: 0.844 |  Val. Acc: 73.90%\nEpoch: 19 | Epoch Time: 0m 8s\n\tTrain Loss: 0.477 | Train Acc: 82.86%\n\t Val. Loss: 0.892 |  Val. Acc: 73.84%\nEpoch: 20 | Epoch Time: 0m 7s\n\tTrain Loss: 0.444 | Train Acc: 84.36%\n\t Val. Loss: 0.805 |  Val. Acc: 74.55%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Задаем оптимизатор, шедулер, устанавливаем weight_decay > 0 для L2-регуляризации в Adam\n\nlearning_rate = 1e-3\nlstm_bi_optimizer = optim.Adam(lstm_bi_model.parameters(), lr=learning_rate, weight_decay=1e-5)\nlstm_bi_scheduler = lr_scheduler.LinearLR(lstm_bi_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:25:34.467822Z","iopub.execute_input":"2023-09-28T16:25:34.468229Z","iopub.status.idle":"2023-09-28T16:25:34.477562Z","shell.execute_reply.started":"2023-09-28T16:25:34.468164Z","shell.execute_reply":"2023-09-28T16:25:34.476315Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# Задаем функцию потерь для LSTM-Bidirectional модели, переносим на device, обучаем модель\n\nlstm_bi_criterion = nn.CrossEntropyLoss()\nlstm_bi_criterion = lstm_bi_criterion.to(device)\n\ntrain_model(train_dataloader,\n            lstm_bi_model,\n            'lstm_bi',\n            lstm_bi_optimizer,\n            lstm_bi_scheduler,\n            lstm_bi_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:25:34.479804Z","iopub.execute_input":"2023-09-28T16:25:34.480251Z","iopub.status.idle":"2023-09-28T16:29:57.446391Z","shell.execute_reply.started":"2023-09-28T16:25:34.480213Z","shell.execute_reply":"2023-09-28T16:29:57.445167Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 13s\n\tTrain Loss: 1.478 | Train Acc: 33.47%\n\t Val. Loss: 1.372 |  Val. Acc: 39.70%\nEpoch: 02 | Epoch Time: 0m 12s\n\tTrain Loss: 1.345 | Train Acc: 40.98%\n\t Val. Loss: 1.268 |  Val. Acc: 46.11%\nEpoch: 03 | Epoch Time: 0m 13s\n\tTrain Loss: 1.248 | Train Acc: 46.60%\n\t Val. Loss: 1.225 |  Val. Acc: 48.59%\nEpoch: 04 | Epoch Time: 0m 13s\n\tTrain Loss: 1.163 | Train Acc: 50.83%\n\t Val. Loss: 1.165 |  Val. Acc: 50.96%\nEpoch: 05 | Epoch Time: 0m 13s\n\tTrain Loss: 1.093 | Train Acc: 54.44%\n\t Val. Loss: 1.119 |  Val. Acc: 53.53%\nEpoch: 06 | Epoch Time: 0m 13s\n\tTrain Loss: 1.020 | Train Acc: 58.25%\n\t Val. Loss: 1.082 |  Val. Acc: 57.48%\nEpoch: 07 | Epoch Time: 0m 13s\n\tTrain Loss: 0.963 | Train Acc: 60.76%\n\t Val. Loss: 1.082 |  Val. Acc: 59.21%\nEpoch: 08 | Epoch Time: 0m 13s\n\tTrain Loss: 0.913 | Train Acc: 63.64%\n\t Val. Loss: 0.988 |  Val. Acc: 62.38%\nEpoch: 09 | Epoch Time: 0m 13s\n\tTrain Loss: 0.859 | Train Acc: 66.00%\n\t Val. Loss: 0.890 |  Val. Acc: 66.08%\nEpoch: 10 | Epoch Time: 0m 13s\n\tTrain Loss: 0.808 | Train Acc: 68.77%\n\t Val. Loss: 0.874 |  Val. Acc: 67.51%\nEpoch: 11 | Epoch Time: 0m 13s\n\tTrain Loss: 0.761 | Train Acc: 70.49%\n\t Val. Loss: 0.857 |  Val. Acc: 69.48%\nEpoch: 12 | Epoch Time: 0m 13s\n\tTrain Loss: 0.711 | Train Acc: 72.66%\n\t Val. Loss: 0.850 |  Val. Acc: 68.73%\nEpoch: 13 | Epoch Time: 0m 13s\n\tTrain Loss: 0.669 | Train Acc: 74.85%\n\t Val. Loss: 0.838 |  Val. Acc: 70.73%\nEpoch: 14 | Epoch Time: 0m 13s\n\tTrain Loss: 0.623 | Train Acc: 76.33%\n\t Val. Loss: 0.841 |  Val. Acc: 71.78%\nEpoch: 15 | Epoch Time: 0m 13s\n\tTrain Loss: 0.578 | Train Acc: 78.41%\n\t Val. Loss: 0.825 |  Val. Acc: 72.45%\nEpoch: 16 | Epoch Time: 0m 13s\n\tTrain Loss: 0.538 | Train Acc: 80.18%\n\t Val. Loss: 0.851 |  Val. Acc: 72.98%\nEpoch: 17 | Epoch Time: 0m 13s\n\tTrain Loss: 0.500 | Train Acc: 81.37%\n\t Val. Loss: 0.839 |  Val. Acc: 73.46%\nEpoch: 18 | Epoch Time: 0m 13s\n\tTrain Loss: 0.454 | Train Acc: 83.21%\n\t Val. Loss: 0.824 |  Val. Acc: 73.90%\nEpoch: 19 | Epoch Time: 0m 13s\n\tTrain Loss: 0.415 | Train Acc: 84.98%\n\t Val. Loss: 0.909 |  Val. Acc: 73.50%\nEpoch: 20 | Epoch Time: 0m 13s\n\tTrain Loss: 0.387 | Train Acc: 85.72%\n\t Val. Loss: 0.899 |  Val. Acc: 74.12%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Задаем оптимизатор, шедулер, устанавливаем weight_decay > 0 для L2-регуляризации в Adam\n\nlearning_rate = 1e-3\ngru_optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate, weight_decay=1e-5)\ngru_scheduler = lr_scheduler.LinearLR(gru_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:29:57.448030Z","iopub.execute_input":"2023-09-28T16:29:57.449482Z","iopub.status.idle":"2023-09-28T16:29:57.456380Z","shell.execute_reply.started":"2023-09-28T16:29:57.449445Z","shell.execute_reply":"2023-09-28T16:29:57.455323Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# Задаем функцию потерь для GRU модели, переносим на device, обучаем модель\n\ngru_criterion = nn.CrossEntropyLoss()\ngru_criterion = gru_criterion.to(device)\n\ntrain_model(train_dataloader,\n            gru_model,\n            'gru',\n            gru_optimizer,\n            gru_scheduler,\n            gru_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:29:57.458115Z","iopub.execute_input":"2023-09-28T16:29:57.458550Z","iopub.status.idle":"2023-09-28T16:32:22.905328Z","shell.execute_reply.started":"2023-09-28T16:29:57.458515Z","shell.execute_reply":"2023-09-28T16:32:22.904251Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 7s\n\tTrain Loss: 1.567 | Train Acc: 27.29%\n\t Val. Loss: 1.559 |  Val. Acc: 29.20%\nEpoch: 02 | Epoch Time: 0m 7s\n\tTrain Loss: 1.509 | Train Acc: 30.66%\n\t Val. Loss: 1.468 |  Val. Acc: 33.96%\nEpoch: 03 | Epoch Time: 0m 7s\n\tTrain Loss: 1.394 | Train Acc: 38.00%\n\t Val. Loss: 1.279 |  Val. Acc: 46.01%\nEpoch: 04 | Epoch Time: 0m 7s\n\tTrain Loss: 1.247 | Train Acc: 46.92%\n\t Val. Loss: 1.168 |  Val. Acc: 52.04%\nEpoch: 05 | Epoch Time: 0m 7s\n\tTrain Loss: 1.147 | Train Acc: 52.04%\n\t Val. Loss: 1.078 |  Val. Acc: 56.44%\nEpoch: 06 | Epoch Time: 0m 7s\n\tTrain Loss: 1.048 | Train Acc: 57.16%\n\t Val. Loss: 1.035 |  Val. Acc: 58.23%\nEpoch: 07 | Epoch Time: 0m 7s\n\tTrain Loss: 0.970 | Train Acc: 60.88%\n\t Val. Loss: 0.946 |  Val. Acc: 63.39%\nEpoch: 08 | Epoch Time: 0m 7s\n\tTrain Loss: 0.895 | Train Acc: 64.79%\n\t Val. Loss: 0.923 |  Val. Acc: 65.17%\nEpoch: 09 | Epoch Time: 0m 7s\n\tTrain Loss: 0.827 | Train Acc: 67.74%\n\t Val. Loss: 0.864 |  Val. Acc: 68.04%\nEpoch: 10 | Epoch Time: 0m 7s\n\tTrain Loss: 0.756 | Train Acc: 70.85%\n\t Val. Loss: 0.822 |  Val. Acc: 70.26%\nEpoch: 11 | Epoch Time: 0m 7s\n\tTrain Loss: 0.712 | Train Acc: 72.97%\n\t Val. Loss: 0.819 |  Val. Acc: 71.11%\nEpoch: 12 | Epoch Time: 0m 7s\n\tTrain Loss: 0.653 | Train Acc: 75.39%\n\t Val. Loss: 0.801 |  Val. Acc: 72.02%\nEpoch: 13 | Epoch Time: 0m 7s\n\tTrain Loss: 0.602 | Train Acc: 77.37%\n\t Val. Loss: 0.831 |  Val. Acc: 72.69%\nEpoch: 14 | Epoch Time: 0m 7s\n\tTrain Loss: 0.558 | Train Acc: 79.36%\n\t Val. Loss: 0.796 |  Val. Acc: 73.45%\nEpoch: 15 | Epoch Time: 0m 7s\n\tTrain Loss: 0.515 | Train Acc: 81.04%\n\t Val. Loss: 0.798 |  Val. Acc: 74.32%\nEpoch: 16 | Epoch Time: 0m 7s\n\tTrain Loss: 0.477 | Train Acc: 82.61%\n\t Val. Loss: 0.830 |  Val. Acc: 73.97%\nEpoch: 17 | Epoch Time: 0m 7s\n\tTrain Loss: 0.442 | Train Acc: 84.01%\n\t Val. Loss: 0.862 |  Val. Acc: 74.81%\nEpoch: 18 | Epoch Time: 0m 7s\n\tTrain Loss: 0.407 | Train Acc: 85.32%\n\t Val. Loss: 0.884 |  Val. Acc: 75.23%\nEpoch: 19 | Epoch Time: 0m 7s\n\tTrain Loss: 0.385 | Train Acc: 86.24%\n\t Val. Loss: 0.968 |  Val. Acc: 74.25%\nEpoch: 20 | Epoch Time: 0m 7s\n\tTrain Loss: 0.351 | Train Acc: 87.58%\n\t Val. Loss: 0.940 |  Val. Acc: 75.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Задаем оптимизатор, шедулер, устанавливаем weight_decay > 0 для L2-регуляризации в Adam\n\nlearning_rate = 1e-3\ngru_bi_optimizer = optim.Adam(gru_bi_model.parameters(), lr=learning_rate, weight_decay=1e-5)\ngru_bi_scheduler = lr_scheduler.LinearLR(gru_bi_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:32:22.906830Z","iopub.execute_input":"2023-09-28T16:32:22.907416Z","iopub.status.idle":"2023-09-28T16:32:22.915217Z","shell.execute_reply.started":"2023-09-28T16:32:22.907381Z","shell.execute_reply":"2023-09-28T16:32:22.913884Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# Задаем функцию потерь для GRU-Bidirectional модели, переносим на device, обучаем модель\n\ngru_bi_criterion = nn.CrossEntropyLoss()\ngru_bi_criterion = gru_bi_criterion.to(device)\n\ntrain_model(train_dataloader,\n            gru_bi_model,\n            'gru_bi',\n            gru_bi_optimizer,\n            gru_bi_scheduler,\n            gru_bi_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:32:22.916931Z","iopub.execute_input":"2023-09-28T16:32:22.917438Z","iopub.status.idle":"2023-09-28T16:36:01.167950Z","shell.execute_reply.started":"2023-09-28T16:32:22.917401Z","shell.execute_reply":"2023-09-28T16:36:01.166739Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 10s\n\tTrain Loss: 1.569 | Train Acc: 27.33%\n\t Val. Loss: 1.520 |  Val. Acc: 31.34%\nEpoch: 02 | Epoch Time: 0m 10s\n\tTrain Loss: 1.506 | Train Acc: 31.31%\n\t Val. Loss: 1.447 |  Val. Acc: 35.27%\nEpoch: 03 | Epoch Time: 0m 10s\n\tTrain Loss: 1.364 | Train Acc: 39.65%\n\t Val. Loss: 1.263 |  Val. Acc: 45.11%\nEpoch: 04 | Epoch Time: 0m 10s\n\tTrain Loss: 1.237 | Train Acc: 46.77%\n\t Val. Loss: 1.171 |  Val. Acc: 50.23%\nEpoch: 05 | Epoch Time: 0m 10s\n\tTrain Loss: 1.129 | Train Acc: 53.08%\n\t Val. Loss: 1.055 |  Val. Acc: 57.72%\nEpoch: 06 | Epoch Time: 0m 10s\n\tTrain Loss: 1.035 | Train Acc: 57.42%\n\t Val. Loss: 1.001 |  Val. Acc: 60.32%\nEpoch: 07 | Epoch Time: 0m 10s\n\tTrain Loss: 0.954 | Train Acc: 61.38%\n\t Val. Loss: 0.932 |  Val. Acc: 63.34%\nEpoch: 08 | Epoch Time: 0m 11s\n\tTrain Loss: 0.885 | Train Acc: 64.82%\n\t Val. Loss: 0.910 |  Val. Acc: 65.00%\nEpoch: 09 | Epoch Time: 0m 10s\n\tTrain Loss: 0.820 | Train Acc: 67.89%\n\t Val. Loss: 0.842 |  Val. Acc: 68.27%\nEpoch: 10 | Epoch Time: 0m 10s\n\tTrain Loss: 0.764 | Train Acc: 70.42%\n\t Val. Loss: 0.859 |  Val. Acc: 69.72%\nEpoch: 11 | Epoch Time: 0m 10s\n\tTrain Loss: 0.704 | Train Acc: 73.06%\n\t Val. Loss: 0.831 |  Val. Acc: 69.71%\nEpoch: 12 | Epoch Time: 0m 10s\n\tTrain Loss: 0.649 | Train Acc: 75.07%\n\t Val. Loss: 0.806 |  Val. Acc: 71.60%\nEpoch: 13 | Epoch Time: 0m 10s\n\tTrain Loss: 0.602 | Train Acc: 77.29%\n\t Val. Loss: 0.838 |  Val. Acc: 72.13%\nEpoch: 14 | Epoch Time: 0m 11s\n\tTrain Loss: 0.554 | Train Acc: 79.42%\n\t Val. Loss: 0.785 |  Val. Acc: 73.29%\nEpoch: 15 | Epoch Time: 0m 10s\n\tTrain Loss: 0.515 | Train Acc: 80.94%\n\t Val. Loss: 0.827 |  Val. Acc: 73.89%\nEpoch: 16 | Epoch Time: 0m 10s\n\tTrain Loss: 0.479 | Train Acc: 82.29%\n\t Val. Loss: 0.825 |  Val. Acc: 73.98%\nEpoch: 17 | Epoch Time: 0m 10s\n\tTrain Loss: 0.443 | Train Acc: 83.66%\n\t Val. Loss: 0.842 |  Val. Acc: 74.22%\nEpoch: 18 | Epoch Time: 0m 10s\n\tTrain Loss: 0.405 | Train Acc: 85.29%\n\t Val. Loss: 0.877 |  Val. Acc: 74.62%\nEpoch: 19 | Epoch Time: 0m 11s\n\tTrain Loss: 0.379 | Train Acc: 86.29%\n\t Val. Loss: 0.984 |  Val. Acc: 73.89%\nEpoch: 20 | Epoch Time: 0m 10s\n\tTrain Loss: 0.346 | Train Acc: 87.79%\n\t Val. Loss: 1.004 |  Val. Acc: 74.29%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Создаем словарь для сравнения моделей\n\nmodels_comparison = {}","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:01.169640Z","iopub.execute_input":"2023-09-28T16:36:01.170307Z","iopub.status.idle":"2023-09-28T16:36:01.177384Z","shell.execute_reply.started":"2023-09-28T16:36:01.170267Z","shell.execute_reply":"2023-09-28T16:36:01.176397Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Загружаем лучшую версию LSTM модели и оцениваем ее на test датасете\n\nlstm_model.load_state_dict(torch.load('best-model_lstm.pt'))\n\ntest_loss, test_acc = evaluate(lstm_model, test_dataloader, lstm_criterion)\n\nmodels_comparison['lstm'] = {'Test Loss': test_loss, 'Test Acc': test_acc}\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:01.179871Z","iopub.execute_input":"2023-09-28T16:36:01.180254Z","iopub.status.idle":"2023-09-28T16:36:02.152050Z","shell.execute_reply.started":"2023-09-28T16:36:01.180213Z","shell.execute_reply":"2023-09-28T16:36:02.150933Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Test Loss: 0.787 | Test Acc: 73.34%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загружаем лучшую версию LSTM-Bidirectional модели и оцениваем ее на test датасете\n\nlstm_bi_model.load_state_dict(torch.load('best-model_lstm_bi.pt'))\n\ntest_loss, test_acc = evaluate(lstm_bi_model, test_dataloader, lstm_bi_criterion)\n\nmodels_comparison['lstm_bi'] = {'Test Loss': test_loss, 'Test Acc': test_acc}\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:02.153612Z","iopub.execute_input":"2023-09-28T16:36:02.155489Z","iopub.status.idle":"2023-09-28T16:36:03.573284Z","shell.execute_reply.started":"2023-09-28T16:36:02.155449Z","shell.execute_reply":"2023-09-28T16:36:03.572160Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Test Loss: 0.824 | Test Acc: 73.90%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загружаем лучшую версию GRU модели и оцениваем ее на test датасете\n\ngru_model.load_state_dict(torch.load('best-model_gru.pt'))\n\ntest_loss, test_acc = evaluate(gru_model, test_dataloader, gru_criterion)\n\nmodels_comparison['gru'] = {'Test Loss': test_loss, 'Test Acc': test_acc}\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:03.574980Z","iopub.execute_input":"2023-09-28T16:36:03.575729Z","iopub.status.idle":"2023-09-28T16:36:04.458047Z","shell.execute_reply.started":"2023-09-28T16:36:03.575689Z","shell.execute_reply":"2023-09-28T16:36:04.457041Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Test Loss: 0.796 | Test Acc: 73.45%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загружаем лучшую версию GRU-Bidirectional модели и оцениваем ее на test датасете\n\ngru_bi_model.load_state_dict(torch.load('best-model_gru_bi.pt'))\n\ntest_loss, test_acc = evaluate(gru_bi_model, test_dataloader, gru_bi_criterion)\n\nmodels_comparison['gru_bi'] = {'Test Loss': test_loss, 'Test Acc': test_acc}\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:04.460259Z","iopub.execute_input":"2023-09-28T16:36:04.461000Z","iopub.status.idle":"2023-09-28T16:36:05.586405Z","shell.execute_reply.started":"2023-09-28T16:36:04.460956Z","shell.execute_reply":"2023-09-28T16:36:05.585050Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Test Loss: 0.785 | Test Acc: 73.29%\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.DataFrame.from_dict(models_comparison, orient='index')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:36:05.588811Z","iopub.execute_input":"2023-09-28T16:36:05.589543Z","iopub.status.idle":"2023-09-28T16:36:05.602656Z","shell.execute_reply.started":"2023-09-28T16:36:05.589507Z","shell.execute_reply":"2023-09-28T16:36:05.601463Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"         Test Loss  Test Acc\nlstm      0.787253  0.733390\nlstm_bi   0.823891  0.739006\ngru       0.795801  0.734524\ngru_bi    0.785351  0.732906","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Test Loss</th>\n      <th>Test Acc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>lstm</th>\n      <td>0.787253</td>\n      <td>0.733390</td>\n    </tr>\n    <tr>\n      <th>lstm_bi</th>\n      <td>0.823891</td>\n      <td>0.739006</td>\n    </tr>\n    <tr>\n      <th>gru</th>\n      <td>0.795801</td>\n      <td>0.734524</td>\n    </tr>\n    <tr>\n      <th>gru_bi</th>\n      <td>0.785351</td>\n      <td>0.732906</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. Получение результата для submit","metadata":{}},{"cell_type":"code","source":"# Создаем итераторы-загрузочники для полного датасета и проверяем работу итератора\n\nBATCH_SIZE = 64\n\ncomplete_dataloader = DataLoader(complete_data, batch_size=BATCH_SIZE, collate_fn=collate_batch, shuffle=True)\n\nprint('Complete')\nfor batch in complete_dataloader:\n    print(f'Text matrix size: {batch[0].size()}')\n    print(f'Target vector size: {batch[1].size()}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:39:33.161663Z","iopub.execute_input":"2023-09-28T16:39:33.162064Z","iopub.status.idle":"2023-09-28T16:39:33.179433Z","shell.execute_reply.started":"2023-09-28T16:39:33.162031Z","shell.execute_reply":"2023-09-28T16:39:33.178325Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"Complete\nText matrix size: torch.Size([64, 39])\nTarget vector size: torch.Size([64])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Задаем гиперпараметры LSTM-Bidirectional модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = len(class_to_idx)\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.4\n\nBM1_model = LSTMClassifier(INPUT_DIM,\n                           EMBEDDING_DIM,\n                           HIDDEN_DIM,\n                           OUTPUT_DIM,\n                           N_LAYERS,\n                           BIDIRECTIONAL,\n                           DROPOUT)\nBM1_model = BM1_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:39:33.308864Z","iopub.execute_input":"2023-09-28T16:39:33.309245Z","iopub.status.idle":"2023-09-28T16:39:33.404004Z","shell.execute_reply.started":"2023-09-28T16:39:33.309186Z","shell.execute_reply":"2023-09-28T16:39:33.402841Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Задаем оптимизатор и функцию потерь для LSTM-Bidirectional модели, переносим на device, обучаем модель\n# В данном случае значения Val. Acc не имеют никакого смысла т.к. valid датасет входит в состав полного датасета, на котором обучается модель\n\nlearning_rate = 1e-3\nBM1_optimizer = optim.Adam(BM1_model.parameters(), lr=learning_rate, weight_decay=1e-5)\nBM1_scheduler = lr_scheduler.LinearLR(BM1_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:39:33.467655Z","iopub.execute_input":"2023-09-28T16:39:33.468324Z","iopub.status.idle":"2023-09-28T16:39:33.475073Z","shell.execute_reply.started":"2023-09-28T16:39:33.468283Z","shell.execute_reply":"2023-09-28T16:39:33.473844Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"BM1_criterion = nn.CrossEntropyLoss()\nBM1_criterion = lstm_bi_criterion.to(device)\n\ntrain_model(complete_dataloader,\n            BM1_model,\n            'BM1',\n            BM1_optimizer,\n            BM1_scheduler,\n            BM1_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:39:33.636297Z","iopub.execute_input":"2023-09-28T16:39:33.636661Z","iopub.status.idle":"2023-09-28T16:45:41.051345Z","shell.execute_reply.started":"2023-09-28T16:39:33.636631Z","shell.execute_reply":"2023-09-28T16:45:41.050229Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 18s\n\tTrain Loss: 1.460 | Train Acc: 34.09%\n\t Val. Loss: 1.339 |  Val. Acc: 41.72%\nEpoch: 02 | Epoch Time: 0m 18s\n\tTrain Loss: 1.305 | Train Acc: 43.52%\n\t Val. Loss: 1.154 |  Val. Acc: 51.78%\nEpoch: 03 | Epoch Time: 0m 18s\n\tTrain Loss: 1.184 | Train Acc: 50.04%\n\t Val. Loss: 1.039 |  Val. Acc: 58.41%\nEpoch: 04 | Epoch Time: 0m 18s\n\tTrain Loss: 1.081 | Train Acc: 55.45%\n\t Val. Loss: 0.974 |  Val. Acc: 61.00%\nEpoch: 05 | Epoch Time: 0m 18s\n\tTrain Loss: 0.983 | Train Acc: 60.39%\n\t Val. Loss: 0.866 |  Val. Acc: 66.01%\nEpoch: 06 | Epoch Time: 0m 18s\n\tTrain Loss: 0.907 | Train Acc: 63.99%\n\t Val. Loss: 0.747 |  Val. Acc: 71.88%\nEpoch: 07 | Epoch Time: 0m 18s\n\tTrain Loss: 0.837 | Train Acc: 67.49%\n\t Val. Loss: 0.650 |  Val. Acc: 75.53%\nEpoch: 08 | Epoch Time: 0m 18s\n\tTrain Loss: 0.774 | Train Acc: 70.34%\n\t Val. Loss: 0.602 |  Val. Acc: 77.92%\nEpoch: 09 | Epoch Time: 0m 18s\n\tTrain Loss: 0.716 | Train Acc: 72.68%\n\t Val. Loss: 0.533 |  Val. Acc: 80.72%\nEpoch: 10 | Epoch Time: 0m 18s\n\tTrain Loss: 0.662 | Train Acc: 75.05%\n\t Val. Loss: 0.478 |  Val. Acc: 83.00%\nEpoch: 11 | Epoch Time: 0m 18s\n\tTrain Loss: 0.602 | Train Acc: 77.86%\n\t Val. Loss: 0.417 |  Val. Acc: 85.34%\nEpoch: 12 | Epoch Time: 0m 18s\n\tTrain Loss: 0.549 | Train Acc: 80.05%\n\t Val. Loss: 0.379 |  Val. Acc: 86.70%\nEpoch: 13 | Epoch Time: 0m 18s\n\tTrain Loss: 0.504 | Train Acc: 81.84%\n\t Val. Loss: 0.351 |  Val. Acc: 88.22%\nEpoch: 14 | Epoch Time: 0m 18s\n\tTrain Loss: 0.452 | Train Acc: 83.69%\n\t Val. Loss: 0.281 |  Val. Acc: 90.25%\nEpoch: 15 | Epoch Time: 0m 18s\n\tTrain Loss: 0.414 | Train Acc: 85.21%\n\t Val. Loss: 0.262 |  Val. Acc: 90.90%\nEpoch: 16 | Epoch Time: 0m 18s\n\tTrain Loss: 0.372 | Train Acc: 86.76%\n\t Val. Loss: 0.217 |  Val. Acc: 92.62%\nEpoch: 17 | Epoch Time: 0m 18s\n\tTrain Loss: 0.339 | Train Acc: 87.93%\n\t Val. Loss: 0.191 |  Val. Acc: 93.49%\nEpoch: 18 | Epoch Time: 0m 18s\n\tTrain Loss: 0.313 | Train Acc: 88.98%\n\t Val. Loss: 0.163 |  Val. Acc: 94.36%\nEpoch: 19 | Epoch Time: 0m 18s\n\tTrain Loss: 0.283 | Train Acc: 90.19%\n\t Val. Loss: 0.168 |  Val. Acc: 94.23%\nEpoch: 20 | Epoch Time: 0m 18s\n\tTrain Loss: 0.259 | Train Acc: 90.97%\n\t Val. Loss: 0.134 |  Val. Acc: 95.32%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загружаем лучшую версию LSTM-Bidirectional модели\n\nBM1_model.load_state_dict(torch.load('best-model_BM1.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:45:41.053711Z","iopub.execute_input":"2023-09-28T16:45:41.054121Z","iopub.status.idle":"2023-09-28T16:45:41.083172Z","shell.execute_reply.started":"2023-09-28T16:45:41.054081Z","shell.execute_reply":"2023-09-28T16:45:41.081924Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Задаем гиперпараметры GRU модели, создаем модель и переносим ее на device\n\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = len(class_to_idx)\nN_LAYERS = 2\nBIDIRECTIONAL = False\nDROPOUT = 0.4\n\nBM2_model = GRUClassifier(INPUT_DIM,\n                           EMBEDDING_DIM,\n                           HIDDEN_DIM,\n                           OUTPUT_DIM,\n                           N_LAYERS,\n                           BIDIRECTIONAL,\n                           DROPOUT)\nBM2_model = BM2_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:45:41.085052Z","iopub.execute_input":"2023-09-28T16:45:41.085505Z","iopub.status.idle":"2023-09-28T16:45:41.165989Z","shell.execute_reply.started":"2023-09-28T16:45:41.085467Z","shell.execute_reply":"2023-09-28T16:45:41.164809Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"# Задаем оптимизатор и функцию потерь для GRU модели, переносим на device, обучаем модель\n# В данном случае значения Val. Acc не имеют никакого смысла т.к. valid датасет входит в состав полного датасета, на котором обучается модель\n\nlearning_rate = 1e-3\nBM2_optimizer = optim.Adam(BM2_model.parameters(), lr=learning_rate, weight_decay=1e-5)\nBM2_scheduler = lr_scheduler.LinearLR(BM2_optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:45:41.169169Z","iopub.execute_input":"2023-09-28T16:45:41.169601Z","iopub.status.idle":"2023-09-28T16:45:41.176499Z","shell.execute_reply.started":"2023-09-28T16:45:41.169563Z","shell.execute_reply":"2023-09-28T16:45:41.175486Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"BM2_criterion = nn.CrossEntropyLoss()\nBM2_criterion = lstm_bi_criterion.to(device)\n\ntrain_model(complete_dataloader,\n            BM2_model,\n            'BM2',\n            BM2_optimizer,\n            BM2_scheduler,\n            BM2_criterion,\n            N_EPOCHS\n           )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:45:41.177893Z","iopub.execute_input":"2023-09-28T16:45:41.180558Z","iopub.status.idle":"2023-09-28T16:49:01.710728Z","shell.execute_reply.started":"2023-09-28T16:45:41.180518Z","shell.execute_reply":"2023-09-28T16:49:01.709692Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 9s\n\tTrain Loss: 1.558 | Train Acc: 27.85%\n\t Val. Loss: 1.502 |  Val. Acc: 30.61%\nEpoch: 02 | Epoch Time: 0m 9s\n\tTrain Loss: 1.431 | Train Acc: 35.74%\n\t Val. Loss: 1.282 |  Val. Acc: 43.51%\nEpoch: 03 | Epoch Time: 0m 10s\n\tTrain Loss: 1.256 | Train Acc: 46.06%\n\t Val. Loss: 1.110 |  Val. Acc: 53.73%\nEpoch: 04 | Epoch Time: 0m 9s\n\tTrain Loss: 1.128 | Train Acc: 52.91%\n\t Val. Loss: 0.944 |  Val. Acc: 62.59%\nEpoch: 05 | Epoch Time: 0m 9s\n\tTrain Loss: 1.010 | Train Acc: 59.31%\n\t Val. Loss: 0.844 |  Val. Acc: 67.34%\nEpoch: 06 | Epoch Time: 0m 10s\n\tTrain Loss: 0.914 | Train Acc: 63.41%\n\t Val. Loss: 0.747 |  Val. Acc: 71.90%\nEpoch: 07 | Epoch Time: 0m 9s\n\tTrain Loss: 0.823 | Train Acc: 67.83%\n\t Val. Loss: 0.649 |  Val. Acc: 76.73%\nEpoch: 08 | Epoch Time: 0m 9s\n\tTrain Loss: 0.749 | Train Acc: 71.27%\n\t Val. Loss: 0.559 |  Val. Acc: 79.93%\nEpoch: 09 | Epoch Time: 0m 10s\n\tTrain Loss: 0.683 | Train Acc: 74.33%\n\t Val. Loss: 0.502 |  Val. Acc: 82.69%\nEpoch: 10 | Epoch Time: 0m 9s\n\tTrain Loss: 0.625 | Train Acc: 76.80%\n\t Val. Loss: 0.456 |  Val. Acc: 84.07%\nEpoch: 11 | Epoch Time: 0m 9s\n\tTrain Loss: 0.577 | Train Acc: 78.76%\n\t Val. Loss: 0.399 |  Val. Acc: 86.31%\nEpoch: 12 | Epoch Time: 0m 10s\n\tTrain Loss: 0.527 | Train Acc: 80.88%\n\t Val. Loss: 0.350 |  Val. Acc: 87.97%\nEpoch: 13 | Epoch Time: 0m 9s\n\tTrain Loss: 0.480 | Train Acc: 82.64%\n\t Val. Loss: 0.323 |  Val. Acc: 88.98%\nEpoch: 14 | Epoch Time: 0m 9s\n\tTrain Loss: 0.447 | Train Acc: 83.86%\n\t Val. Loss: 0.275 |  Val. Acc: 90.92%\nEpoch: 15 | Epoch Time: 0m 10s\n\tTrain Loss: 0.410 | Train Acc: 85.27%\n\t Val. Loss: 0.283 |  Val. Acc: 90.19%\nEpoch: 16 | Epoch Time: 0m 9s\n\tTrain Loss: 0.370 | Train Acc: 86.87%\n\t Val. Loss: 0.221 |  Val. Acc: 92.43%\nEpoch: 17 | Epoch Time: 0m 9s\n\tTrain Loss: 0.345 | Train Acc: 87.59%\n\t Val. Loss: 0.209 |  Val. Acc: 93.01%\nEpoch: 18 | Epoch Time: 0m 10s\n\tTrain Loss: 0.317 | Train Acc: 88.81%\n\t Val. Loss: 0.185 |  Val. Acc: 93.89%\nEpoch: 19 | Epoch Time: 0m 10s\n\tTrain Loss: 0.294 | Train Acc: 89.66%\n\t Val. Loss: 0.179 |  Val. Acc: 94.28%\nEpoch: 20 | Epoch Time: 0m 9s\n\tTrain Loss: 0.269 | Train Acc: 90.61%\n\t Val. Loss: 0.154 |  Val. Acc: 95.02%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Загружаем лучшую версию LSTM-Bidirectional модели\n\nBM2_model.load_state_dict(torch.load('best-model_BM2.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:01.712377Z","iopub.execute_input":"2023-09-28T16:49:01.713515Z","iopub.status.idle":"2023-09-28T16:49:01.737929Z","shell.execute_reply.started":"2023-09-28T16:49:01.713480Z","shell.execute_reply":"2023-09-28T16:49:01.736754Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Загружаем тексты для test датасета\n\ntest_df = pd.read_csv(f'{work_dir}/test.csv')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:01.739822Z","iopub.execute_input":"2023-09-28T16:49:01.740239Z","iopub.status.idle":"2023-09-28T16:49:01.793356Z","shell.execute_reply.started":"2023-09-28T16:49:01.740185Z","shell.execute_reply":"2023-09-28T16:49:01.792260Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"                                        id  \\\n0     787bc85b-20d4-46d8-84a0-562a2527f684   \n1     17e934cd-ba94-4d4f-9ac0-ead202abe241   \n2     5914534b-2b0f-4de8-bb8a-e25587697e0d   \n3     cdf06cfe-29ae-48ee-ac6d-be448103ba45   \n4     aff63979-0256-4fb9-a2d9-86a3d3ca5470   \n...                                    ...   \n3793  65712d27-5c41-4863-b74f-0bd66199b7df   \n3794  9fd189c5-e79c-49d7-8985-576450a4e6e3   \n3795  3a06785f-6f9b-4f4d-9880-22562ad3e296   \n3796  dd29ff09-9bc2-40f4-8201-4b6361aca760   \n3797  8d09ea68-a130-4f3a-8777-f821b354542d   \n\n                                                   Text  \n0     TRENDING: New Yorkers encounter empty supermar...  \n1     When I couldn't find hand sanitizer at Fred Me...  \n2     Find out how you can protect yourself and love...  \n3     #Panic buying hits #NewYork City as anxious sh...  \n4     #toiletpaper #dunnypaper #coronavirus #coronav...  \n...                                                 ...  \n3793  Meanwhile In A Supermarket in Israel -- People...  \n3794  Did you panic buy a lot of non-perishable item...  \n3795  Asst Prof of Economics @cconces was on @NBCPhi...  \n3796  Gov need to do somethings instead of biar je r...  \n3797  I and @ForestandPaper members are committed to...  \n\n[3798 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n      <td>Find out how you can protect yourself and love...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3793</th>\n      <td>65712d27-5c41-4863-b74f-0bd66199b7df</td>\n      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n    </tr>\n    <tr>\n      <th>3794</th>\n      <td>9fd189c5-e79c-49d7-8985-576450a4e6e3</td>\n      <td>Did you panic buy a lot of non-perishable item...</td>\n    </tr>\n    <tr>\n      <th>3795</th>\n      <td>3a06785f-6f9b-4f4d-9880-22562ad3e296</td>\n      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n    </tr>\n    <tr>\n      <th>3796</th>\n      <td>dd29ff09-9bc2-40f4-8201-4b6361aca760</td>\n      <td>Gov need to do somethings instead of biar je r...</td>\n    </tr>\n    <tr>\n      <th>3797</th>\n      <td>8d09ea68-a130-4f3a-8777-f821b354542d</td>\n      <td>I and @ForestandPaper members are committed to...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3798 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Проверяем на наличие nan\n\nnan_count = test_df.isna().sum().sum()\nnan_count","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:01.794891Z","iopub.execute_input":"2023-09-28T16:49:01.797107Z","iopub.status.idle":"2023-09-28T16:49:01.808044Z","shell.execute_reply.started":"2023-09-28T16:49:01.797060Z","shell.execute_reply":"2023-09-28T16:49:01.806419Z"},"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# Предобрабатываем текст\n\ntest_df['Processed_text'] = test_df['Text'].map(process_text)\ntest_df['Processed_text']","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:01.810395Z","iopub.execute_input":"2023-09-28T16:49:01.811730Z","iopub.status.idle":"2023-09-28T16:49:04.593040Z","shell.execute_reply.started":"2023-09-28T16:49:01.811677Z","shell.execute_reply":"2023-09-28T16:49:04.591864Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3451095874.py:53: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  return BeautifulSoup(input_text, \"lxml\").text\n","output_type":"stream"},{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"0       trending: new yorkers encounter empty supermar...\n1       find hand sanitizer fred meyer, turned #amazon...\n2                   find protect loved ones #coronavirus.\n3       #panic buying hits #newyork city anxious shopp...\n4       #toiletpaper #dunnypaper #coronavirus #coronav...\n                              ...                        \n3793    meanwhile supermarket israel -- people dance s...\n3794    panic buy lot non-perishable items? echo needs...\n3795    asst prof economics @cconces @nbcphiladelphia ...\n3796    gov need somethings instead biar je rakyat ass...\n3797    @forestandpaper members committed safety emplo...\nName: Processed_text, Length: 3798, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Пишем функцию для генерации предсказания\n\ndef make_prediction(input_text, models):\n    predictions = []\n    for model in models:\n        model.eval()\n        processed_text = text_pipeline(input_text)\n        processed_text = torch.tensor(processed_text, dtype=torch.int64).to(device)\n        prediction = model(processed_text.unsqueeze(0), torch.tensor([len(processed_text)]).to(device))\n        predictions.append(prediction)\n    mean_prediction = torch.mean(torch.stack(predictions), 0)\n    return torch.argmax(torch.softmax(mean_prediction, dim=1)).item()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:04.597108Z","iopub.execute_input":"2023-09-28T16:49:04.597580Z","iopub.status.idle":"2023-09-28T16:49:04.605128Z","shell.execute_reply.started":"2023-09-28T16:49:04.597544Z","shell.execute_reply":"2023-09-28T16:49:04.604065Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"models = [BM1_model, BM2_model]\n\ntest_df['pred'] = test_df['Processed_text'].apply(lambda x: make_prediction(x, models=models))\ntest_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:04.606743Z","iopub.execute_input":"2023-09-28T16:49:04.608033Z","iopub.status.idle":"2023-09-28T16:49:20.393967Z","shell.execute_reply.started":"2023-09-28T16:49:04.607997Z","shell.execute_reply":"2023-09-28T16:49:20.392802Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"                                        id  \\\n0     787bc85b-20d4-46d8-84a0-562a2527f684   \n1     17e934cd-ba94-4d4f-9ac0-ead202abe241   \n2     5914534b-2b0f-4de8-bb8a-e25587697e0d   \n3     cdf06cfe-29ae-48ee-ac6d-be448103ba45   \n4     aff63979-0256-4fb9-a2d9-86a3d3ca5470   \n...                                    ...   \n3793  65712d27-5c41-4863-b74f-0bd66199b7df   \n3794  9fd189c5-e79c-49d7-8985-576450a4e6e3   \n3795  3a06785f-6f9b-4f4d-9880-22562ad3e296   \n3796  dd29ff09-9bc2-40f4-8201-4b6361aca760   \n3797  8d09ea68-a130-4f3a-8777-f821b354542d   \n\n                                                   Text  \\\n0     TRENDING: New Yorkers encounter empty supermar...   \n1     When I couldn't find hand sanitizer at Fred Me...   \n2     Find out how you can protect yourself and love...   \n3     #Panic buying hits #NewYork City as anxious sh...   \n4     #toiletpaper #dunnypaper #coronavirus #coronav...   \n...                                                 ...   \n3793  Meanwhile In A Supermarket in Israel -- People...   \n3794  Did you panic buy a lot of non-perishable item...   \n3795  Asst Prof of Economics @cconces was on @NBCPhi...   \n3796  Gov need to do somethings instead of biar je r...   \n3797  I and @ForestandPaper members are committed to...   \n\n                                         Processed_text  pred  \n0     trending: new yorkers encounter empty supermar...     1  \n1     find hand sanitizer fred meyer, turned #amazon...     3  \n2                 find protect loved ones #coronavirus.     4  \n3     #panic buying hits #newyork city anxious shopp...     2  \n4     #toiletpaper #dunnypaper #coronavirus #coronav...     1  \n...                                                 ...   ...  \n3793  meanwhile supermarket israel -- people dance s...     3  \n3794  panic buy lot non-perishable items? echo needs...     1  \n3795  asst prof economics @cconces @nbcphiladelphia ...     2  \n3796  gov need somethings instead biar je rakyat ass...     0  \n3797  @forestandpaper members committed safety emplo...     4  \n\n[3798 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Processed_text</th>\n      <th>pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n      <td>trending: new yorkers encounter empty supermar...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n      <td>find hand sanitizer fred meyer, turned #amazon...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n      <td>Find out how you can protect yourself and love...</td>\n      <td>find protect loved ones #coronavirus.</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n      <td>#panic buying hits #newyork city anxious shopp...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3793</th>\n      <td>65712d27-5c41-4863-b74f-0bd66199b7df</td>\n      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n      <td>meanwhile supermarket israel -- people dance s...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3794</th>\n      <td>9fd189c5-e79c-49d7-8985-576450a4e6e3</td>\n      <td>Did you panic buy a lot of non-perishable item...</td>\n      <td>panic buy lot non-perishable items? echo needs...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3795</th>\n      <td>3a06785f-6f9b-4f4d-9880-22562ad3e296</td>\n      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n      <td>asst prof economics @cconces @nbcphiladelphia ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3796</th>\n      <td>dd29ff09-9bc2-40f4-8201-4b6361aca760</td>\n      <td>Gov need to do somethings instead of biar je r...</td>\n      <td>gov need somethings instead biar je rakyat ass...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3797</th>\n      <td>8d09ea68-a130-4f3a-8777-f821b354542d</td>\n      <td>I and @ForestandPaper members are committed to...</td>\n      <td>@forestandpaper members committed safety emplo...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>3798 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Генерируем датафрейм для сохранения\n\nidx_to_class = {0: 'Extremely Negative',\n                1: 'Negative',\n                2: 'Neutral',\n                3: 'Positive',\n                4: 'Extremely Positive'\n                }\n\ndef convert_idx_to_class(input_label):\n    return idx_to_class[input_label]\n\nres_df = pd.DataFrame()\nres_df['id'] = test_df['id']\nres_df['Sentiment'] = test_df['pred'].map(convert_idx_to_class)\nres_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:20.395782Z","iopub.execute_input":"2023-09-28T16:49:20.396484Z","iopub.status.idle":"2023-09-28T16:49:20.418077Z","shell.execute_reply.started":"2023-09-28T16:49:20.396445Z","shell.execute_reply":"2023-09-28T16:49:20.416717Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"                                        id           Sentiment\n0     787bc85b-20d4-46d8-84a0-562a2527f684            Negative\n1     17e934cd-ba94-4d4f-9ac0-ead202abe241            Positive\n2     5914534b-2b0f-4de8-bb8a-e25587697e0d  Extremely Positive\n3     cdf06cfe-29ae-48ee-ac6d-be448103ba45             Neutral\n4     aff63979-0256-4fb9-a2d9-86a3d3ca5470            Negative\n...                                    ...                 ...\n3793  65712d27-5c41-4863-b74f-0bd66199b7df            Positive\n3794  9fd189c5-e79c-49d7-8985-576450a4e6e3            Negative\n3795  3a06785f-6f9b-4f4d-9880-22562ad3e296             Neutral\n3796  dd29ff09-9bc2-40f4-8201-4b6361aca760  Extremely Negative\n3797  8d09ea68-a130-4f3a-8777-f821b354542d  Extremely Positive\n\n[3798 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n      <td>Extremely Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3793</th>\n      <td>65712d27-5c41-4863-b74f-0bd66199b7df</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3794</th>\n      <td>9fd189c5-e79c-49d7-8985-576450a4e6e3</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>3795</th>\n      <td>3a06785f-6f9b-4f4d-9880-22562ad3e296</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>3796</th>\n      <td>dd29ff09-9bc2-40f4-8201-4b6361aca760</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>3797</th>\n      <td>8d09ea68-a130-4f3a-8777-f821b354542d</td>\n      <td>Extremely Positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>3798 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Сохраняем в csv\n\nres_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:20.420087Z","iopub.execute_input":"2023-09-28T16:49:20.420551Z","iopub.status.idle":"2023-09-28T16:49:20.446985Z","shell.execute_reply.started":"2023-09-28T16:49:20.420509Z","shell.execute_reply":"2023-09-28T16:49:20.445906Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Проверяем формат сохраненного файла\n\npd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T16:49:20.448379Z","iopub.execute_input":"2023-09-28T16:49:20.448707Z","iopub.status.idle":"2023-09-28T16:49:20.470349Z","shell.execute_reply.started":"2023-09-28T16:49:20.448673Z","shell.execute_reply":"2023-09-28T16:49:20.469389Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"                                        id           Sentiment\n0     787bc85b-20d4-46d8-84a0-562a2527f684            Negative\n1     17e934cd-ba94-4d4f-9ac0-ead202abe241            Positive\n2     5914534b-2b0f-4de8-bb8a-e25587697e0d  Extremely Positive\n3     cdf06cfe-29ae-48ee-ac6d-be448103ba45             Neutral\n4     aff63979-0256-4fb9-a2d9-86a3d3ca5470            Negative\n...                                    ...                 ...\n3793  65712d27-5c41-4863-b74f-0bd66199b7df            Positive\n3794  9fd189c5-e79c-49d7-8985-576450a4e6e3            Negative\n3795  3a06785f-6f9b-4f4d-9880-22562ad3e296             Neutral\n3796  dd29ff09-9bc2-40f4-8201-4b6361aca760  Extremely Negative\n3797  8d09ea68-a130-4f3a-8777-f821b354542d  Extremely Positive\n\n[3798 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n      <td>Extremely Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3793</th>\n      <td>65712d27-5c41-4863-b74f-0bd66199b7df</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3794</th>\n      <td>9fd189c5-e79c-49d7-8985-576450a4e6e3</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>3795</th>\n      <td>3a06785f-6f9b-4f4d-9880-22562ad3e296</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>3796</th>\n      <td>dd29ff09-9bc2-40f4-8201-4b6361aca760</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>3797</th>\n      <td>8d09ea68-a130-4f3a-8777-f821b354542d</td>\n      <td>Extremely Positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>3798 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
